
import streamlit as st
import requests
import openai
import logging
from bs4 import BeautifulSoup
from prompt_config import SUMMARY_PROMPT, SEARCH_QUERY_PROMPT, EVALUATION_PROMPT
from default_config import DEFAULT_API_KEY, DEFAULT_BASE_URL, DEFAULT_MODEL_NAME

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- UI: ä¾§è¾¹æ é…ç½® ---
with st.sidebar:
    st.header("âš™ï¸ AI é…ç½®")
    
    # ä½¿ç”¨ session_state æ¥æŒä¹…åŒ–ç”¨æˆ·è¾“å…¥
    if 'api_key' not in st.session_state:
        st.session_state.api_key = DEFAULT_API_KEY
    if 'base_url' not in st.session_state:
        st.session_state.base_url = DEFAULT_BASE_URL
    if 'model_name' not in st.session_state:
        st.session_state.model_name = DEFAULT_MODEL_NAME

    st.session_state.api_key = st.text_input("API Key", type="password", value=st.session_state.api_key)
    st.session_state.base_url = st.text_input("Base URL (å¯é€‰)", value=st.session_state.base_url)
    st.session_state.model_name = st.text_input("æ¨¡å‹åç§°", value=st.session_state.model_name)

    # æ–°å¢ï¼šå•æ¬¡æœç´¢ç»“æœæ•°é‡é…ç½®
    if 'max_search_results_per_iteration' not in st.session_state:
        st.session_state.max_search_results_per_iteration = 5
    st.session_state.max_search_results_per_iteration = st.slider(
        "å•æ¬¡æœç´¢ç»“æœæ•°é‡",
        min_value=1,
        max_value=15,
        value=st.session_state.max_search_results_per_iteration,
        step=1,
        help="æ¯æ¬¡è¿­ä»£æœç´¢æ—¶è·å–çš„ç½‘é¡µç»“æœæ•°é‡ã€‚"
    )
    
    st.info("é…ç½®ä¿¡æ¯å°†åœ¨æ­¤ä¼šè¯ä¸­ä¿æŒæœ‰æ•ˆã€‚")

# --- æ ¸å¿ƒé€»è¾‘ ---
def search_the_web(query: str, max_results: int = 5):
    """ä½¿ç”¨ Bing æœç´¢ç½‘é¡µå¹¶è¿”å›æœ€ä½³ç»“æœã€‚"""
    logging.info(f"å¼€å§‹ä¸ºæŸ¥è¯¢ '{query}' è¿›è¡Œ Bing æœç´¢...")
    try:
        search_url = f"https://www.bing.com/search?q={query}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(search_url, headers=headers, timeout=20)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        search_items = soup.find_all('li', class_='b_algo')
        
        results = []
        for item in search_items:
            if len(results) >= max_results:
                break
            title_tag = item.find('h2')
            link_tag = title_tag.find('a') if title_tag else None
            if link_tag and link_tag.get('href'):
                title = link_tag.get_text(strip=True)
                href = link_tag.get('href')
                if href.startswith('http') and 'microsoft.com' not in href:
                    results.append({'title': title, 'href': href})

        logging.info(f"Bing æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆç»“æœã€‚")
        return results

    except requests.RequestException as e:
        logging.error(f"è¯·æ±‚ Bing æœç´¢æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}", exc_info=True)
        return []
    except Exception as e:
        logging.error(f"è§£æ Bing æœç´¢ç»“æœæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        return []

def scrape_website_content(url: str):
    """ä»ç»™å®š URL æŠ“å–ä¸»è¦æ–‡æœ¬å†…å®¹ã€‚"""
    logging.info(f"å¼€å§‹æŠ“å– URL: {url}")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        for script_or_style in soup(['script', 'style']):
            script_or_style.decompose()
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        logging.info(f"æˆåŠŸæŠ“å– URL: {url}")
        return text
    except requests.RequestException as e:
        logging.error(f"æŠ“å– {url} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return None

def generate_search_query_with_llm(user_question: str, api_key: str, model_name: str, base_url: str = None):
    """ä½¿ç”¨ LLM æ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆä¼˜åŒ–çš„æœç´¢è¯å¥ã€‚"""
    logging.info(f"å¼€å§‹ä½¿ç”¨æ¨¡å‹ '{model_name}' ç”Ÿæˆæœç´¢è¯å¥...")
    try:
        client = openai.OpenAI(api_key=api_key, base_url=base_url if base_url else None)
        
        prompt = SEARCH_QUERY_PROMPT.format(user_question=user_question)

        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3, # è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„æœç´¢è¯
        )
        search_query = response.choices[0].message.content.strip()
        logging.info(f"ç”Ÿæˆçš„æœç´¢è¯å¥: {search_query}")
        return search_query

    except Exception as e:
        logging.error(f"ç”Ÿæˆæœç´¢è¯å¥æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        st.error(f"ç”Ÿæˆæœç´¢è¯å¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def summarize_with_llm(user_question: str, scraped_data: list, api_key: str, model_name: str, base_url: str = None):
    """ä½¿ç”¨ LLM ç”Ÿæˆå¸¦å¼•ç”¨æ¥æºçš„æ‘˜è¦ã€‚"""
    logging.info(f"å¼€å§‹ä½¿ç”¨æ¨¡å‹ '{model_name}' ç”Ÿæˆæ‘˜è¦...")
    try:
        client = openai.OpenAI(api_key=api_key, base_url=base_url if base_url else None)
        
        content_for_prompt = ""
        for item in scraped_data:
            content_for_prompt += f"æ¥æº URL: {item['url']}\nå†…å®¹:\n{item['content']}\n\n---\n\n"

        # ä»é…ç½®æ–‡ä»¶åŠ è½½å¹¶æ ¼å¼åŒ– prompt
        prompt = SUMMARY_PROMPT.format(user_question=user_question, content_for_prompt=content_for_prompt)

        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )
        logging.info("æ‘˜è¦ç”ŸæˆæˆåŠŸã€‚")
        return response.choices[0].message.content

    except Exception as e:
        logging.error(f"ç”Ÿæˆæ‘˜è¦æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        st.error(f"ç”Ÿæˆæ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def evaluate_content_sufficiency_with_llm(user_question: str, scraped_data: list, api_key: str, model_name: str, base_url: str = None):
    """ä½¿ç”¨ LLM è¯„ä¼°æŠ“å–åˆ°çš„å†…å®¹æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"""
    logging.info(f"å¼€å§‹ä½¿ç”¨æ¨¡å‹ '{model_name}' è¯„ä¼°å†…å®¹å……è¶³æ€§...")
    try:
        client = openai.OpenAI(api_key=api_key, base_url=base_url if base_url else None)
        
        content_summary = ""
        for item in scraped_data:
            content_summary += f"æ¥æº URL: {item['url']}\nå†…å®¹æ‘˜è¦: {item['content'][:500]}...\n\n" # æå–éƒ¨åˆ†å†…å®¹ä½œä¸ºæ‘˜è¦

        prompt = EVALUATION_PROMPT.format(user_question=user_question, scraped_content_summary=content_summary)

        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.1, # è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„åˆ¤æ–­
        )
        evaluation_result = response.choices[0].message.content.strip().upper()
        logging.info(f"AI å†…å®¹å……è¶³æ€§è¯„ä¼°ç»“æœ: {evaluation_result}")
        return evaluation_result == "YES"

    except Exception as e:
        logging.error(f"è¯„ä¼°å†…å®¹å……è¶³æ€§æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        st.error(f"è¯„ä¼°å†…å®¹å……è¶³æ€§æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

# --- UI é…ç½® ---
st.set_page_config(
    page_title="Easy Search æ˜“æœ",
    page_icon="âœ¨",
    layout="wide"
)

# --- åº”ç”¨æ ‡é¢˜å’Œæè¿° ---
st.title("ğŸ” Easy Search æ˜“æœ")
st.markdown("""
æ¬¢è¿ä½¿ç”¨ Easy Search æ˜“æœï¼è¯·ç”¨è‡ªç„¶è¯­è¨€è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘å°†ä¸ºæ‚¨æœç´¢ç½‘ç»œã€
æ€»ç»“ä¿¡æ¯ï¼Œå¹¶æä¾›æ¸…æ™°ä¸”æœ‰æ¥æºçš„ç­”æ¡ˆã€‚
""")

# --- ä¸»è¦æœç´¢åŒºåŸŸ ---
st.header("æå‡ºæ‚¨çš„é—®é¢˜")

def clear_question():
    st.session_state.user_question_input = ""

cols_question_input = st.columns([0.8, 0.2])
with cols_question_input[0]:
    user_question = st.text_input(
        label="åœ¨æ­¤è¾“å…¥æ‚¨çš„é—®é¢˜:",
        placeholder="ä¾‹å¦‚ï¼šä¸ºä»€ä¹ˆæˆ‘çš„ç¬”è®°æœ¬ç”µè„‘é£æ‰‡å£°éŸ³ç‰¹åˆ«å¤§ï¼Ÿï¼ˆEasy Search æ˜“æœï¼‰",
        label_visibility="collapsed",
        key="user_question_input" # ä¸ºè¾“å…¥æ¡†æ·»åŠ ä¸€ä¸ªkey
    )
with cols_question_input[1]:
    st.button("æ¸…ç©ºé—®é¢˜", on_click=clear_question, help="æ¸…ç©ºé—®é¢˜è¾“å…¥æ¡†")

search_button = st.button("è·å–æ™ºèƒ½ç­”æ¡ˆ")

# --- ç»“æœåŒºåŸŸ ---
cols_answer_header = st.columns([0.9, 0.1]) # è°ƒæ•´æ¯”ä¾‹ï¼Œè®©æŒ‰é’®æ›´ç´§å‡‘
with cols_answer_header[0]:
    st.header("å›ç­”")
with cols_answer_header[1]:
    # æ¸…ç©ºå›ç­”çš„æŒ‰é’®
    def clear_answer():
        st.session_state.answer_content = ""
        st.session_state.show_placeholder = True
        st.session_state.user_question_input = "" # æ¸…ç©ºé—®é¢˜è¾“å…¥æ¡†
        # é‡æ–°æ¸²æŸ“å ä½ç¬¦
        answer_display_area.markdown("<p style='color:grey;'>æ‚¨çš„é—®é¢˜å›ç­”å°†æ˜¾ç¤ºåœ¨æ­¤...</p>", unsafe_allow_html=True)

    st.button("æ¸…ç©ºå›ç­”", on_click=clear_answer, help="æ¸…ç©ºå›ç­”åŒºåŸŸ")

results_area = st.container(border=True)

# åˆå§‹åŒ– session_state ä¸­çš„å›ç­”å†…å®¹å’Œå ä½ç¬¦æ˜¾ç¤ºçŠ¶æ€
if 'answer_content' not in st.session_state:
    st.session_state.answer_content = ""
if 'show_placeholder' not in st.session_state:
    st.session_state.show_placeholder = True

# ç”¨äºæ˜¾ç¤ºå›ç­”çš„åŠ¨æ€åŒºåŸŸ
answer_display_area = results_area.empty()

# æ ¹æ®çŠ¶æ€æ˜¾ç¤ºå ä½ç¬¦æˆ–å®é™…å†…å®¹
if st.session_state.show_placeholder:
    answer_display_area.markdown("<p style='color:grey;'>æ‚¨çš„é—®é¢˜å›ç­”å°†æ˜¾ç¤ºåœ¨æ­¤...</p>", unsafe_allow_html=True)
else:
    answer_display_area.markdown(st.session_state.answer_content)
# results_area.write("æ‚¨çš„æ‘˜è¦ç­”æ¡ˆå°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...")

# --- æŒ‰é’®ç‚¹å‡»é€»è¾‘ ---
if search_button:
    if not st.session_state.api_key:
        results_area.error("è¯·è¾“å…¥æ‚¨çš„ API Keyã€‚")
    elif not user_question:
        results_area.warning("è¿›è¡Œæœç´¢å‰ï¼Œè¯·è¾“å…¥ä¸€ä¸ªé—®é¢˜ã€‚")
    else:
        try:
            # æ¸…ç©ºç»“æœåŒºåŸŸï¼Œå‡†å¤‡æ˜¾ç¤ºè¿›åº¦æ¡å’Œæœ€ç»ˆç»“æœ
            # results_area.empty() # è¿™ä¸€è¡Œä¸å†éœ€è¦ï¼Œå› ä¸º answer_display_area å·²ç»æ§åˆ¶äº†æ˜¾ç¤º
            # æ¯æ¬¡ç‚¹å‡»æœç´¢æŒ‰é’®æ—¶ï¼Œé‡ç½®å›ç­”åŒºåŸŸä¸ºåˆå§‹çŠ¶æ€
            st.session_state.answer_content = ""
            st.session_state.show_placeholder = True
            answer_display_area.markdown("<p style='color:grey;'>æ‚¨çš„é—®é¢˜å›ç­”å°†æ˜¾ç¤ºåœ¨æ­¤...</p>", unsafe_allow_html=True)
            
            # éšè—å ä½ç¬¦ï¼Œå‡†å¤‡æ˜¾ç¤ºè¿›åº¦æ¡
            st.session_state.show_placeholder = False
            progress_text = "æ­£åœ¨åˆå§‹åŒ–..."
            my_bar = results_area.progress(0, text=progress_text)

            try:
                # æ­¥éª¤ 1: ç”Ÿæˆæœç´¢è¯å¥
                my_bar.progress(20, text="æ­£åœ¨åˆ†æç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆæœç´¢è¯å¥...")
                optimized_query = generate_search_query_with_llm(
                    user_question=user_question,
                    api_key=st.session_state.api_key,
                    model_name=st.session_state.model_name,
                    base_url=st.session_state.base_url
                )
                if not optimized_query:
                    results_area.error("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æœç´¢è¯å¥ã€‚è¯·æ£€æŸ¥ AI é…ç½®æˆ–å°è¯•å…¶ä»–é—®é¢˜ã€‚")
                    my_bar.empty()
                    st.stop()
                logging.info(f"AI ç”Ÿæˆçš„æœç´¢è¯å¥: {optimized_query}")

                # æ­¥éª¤ 2: è¿­ä»£æœç´¢å’Œå†…å®¹æŠ“å–ï¼Œç›´åˆ°å†…å®¹å……è¶³æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
                MAX_ITERATIONS = 3 # è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°
                current_iteration = 0
                all_scraped_content = []
                content_sufficient = False
                
                while not content_sufficient and current_iteration < MAX_ITERATIONS:
                    current_iteration += 1
                    my_bar.progress(30 + (current_iteration * 15), text=f"æ­£åœ¨è¿›è¡Œç¬¬ {current_iteration} è½®æœç´¢å’Œå†…å®¹æŠ“å–...")
                    
                    # æ‰§è¡Œç½‘ç»œæœç´¢
                    search_results = search_the_web(optimized_query, max_results=st.session_state.max_search_results_per_iteration * current_iteration) # æ¯æ¬¡è¿­ä»£è·å–æ›´å¤šç»“æœ
                    if not search_results:
                        if current_iteration == 1: # ç¬¬ä¸€æ¬¡æœç´¢å°±æ²¡ç»“æœï¼Œç›´æ¥æŠ¥é”™
                            results_area.error("æœªèƒ½æ‰¾åˆ°ä»»ä½•ç½‘ç»œç»“æœã€‚è¯·å°è¯•å…¶ä»–é—®é¢˜ã€‚")
                            my_bar.empty()
                            st.stop()
                        else: # åç»­è¿­ä»£æ²¡ç»“æœï¼Œè·³å‡ºå¾ªç¯
                            logging.warning(f"ç¬¬ {current_iteration} è½®æœç´¢æœªèƒ½æ‰¾åˆ°æ›´å¤šç½‘ç»œç»“æœã€‚")
                            break
                    logging.info(f"ç¬¬ {current_iteration} è½®æ‰¾åˆ°äº† {len(search_results)} ä¸ªç½‘ç»œç»“æœã€‚")

                    # æŠ“å–ç½‘é¡µå†…å®¹
                    new_scraped_content = []
                    for i, result in enumerate(search_results):
                        url = result['href']
                        # é¿å…é‡å¤æŠ“å–å·²æœ‰çš„ URL
                        if url not in [item['url'] for item in all_scraped_content]:
                            my_bar.progress(30 + (current_iteration * 15) + int(5 * (i + 1) / len(search_results)), text=f"ç¬¬ {current_iteration} è½®æŠ“å–: {url}")
                            content = scrape_website_content(url)
                            if content:
                                new_scraped_content.append({"url": url, "content": content})
                            else:
                                logging.warning(f"æ— æ³•ä» {url} æŠ“å–å†…å®¹")
                    
                    if new_scraped_content:
                        all_scraped_content.extend(new_scraped_content)
                        logging.info(f"ç¬¬ {current_iteration} è½®æˆåŠŸæŠ“å– {len(new_scraped_content)} ä¸ªé¡µé¢ã€‚æ€»è®¡æŠ“å– {len(all_scraped_content)} ä¸ªé¡µé¢ã€‚")
                        
                        # è¯„ä¼°å†…å®¹å……è¶³æ€§
                        my_bar.progress(80, text="æ­£åœ¨è¯„ä¼°å†…å®¹å……è¶³æ€§...")
                        content_sufficient = evaluate_content_sufficiency_with_llm(
                            user_question=user_question,
                            scraped_data=all_scraped_content,
                            api_key=st.session_state.api_key,
                            model_name=st.session_state.model_name,
                            base_url=st.session_state.base_url
                        )
                        if content_sufficient:
                            logging.info("AI è¯„ä¼°å†…å®¹å……è¶³ï¼Œå‡†å¤‡ç”Ÿæˆæ‘˜è¦ã€‚")
                        else:
                            logging.info("AI è¯„ä¼°å†…å®¹ä¸è¶³ï¼Œå°†è¿›è¡Œä¸‹ä¸€è½®æœç´¢ã€‚")
                    else:
                        logging.warning(f"ç¬¬ {current_iteration} è½®æœªèƒ½æŠ“å–åˆ°ä»»ä½•æ–°å†…å®¹ã€‚")
                        break # å¦‚æœæ²¡æœ‰æ–°å†…å®¹è¢«æŠ“å–ï¼Œåˆ™åœæ­¢è¿­ä»£

                if not all_scraped_content:
                    results_area.error("æœªèƒ½ä»ä»»ä½•æœç´¢ç»“æœä¸­æŠ“å–åˆ°æœ‰æ•ˆå†…å®¹ã€‚è¯·å°è¯•å…¶ä»–é—®é¢˜ã€‚")
                    my_bar.empty()
                    st.stop()

                if not content_sufficient:
                    results_area.warning(f"åœ¨ {MAX_ITERATIONS} è½®æœç´¢åï¼ŒAI ä»è®¤ä¸ºå†…å®¹ä¸è¶³ä»¥å®Œå…¨å›ç­”æ‚¨çš„é—®é¢˜ã€‚å°†åŸºäºç°æœ‰å†…å®¹ç”Ÿæˆæ‘˜è¦ã€‚")
                
                # æ­¥éª¤ 3: ç”Ÿæˆæ‘˜è¦
                my_bar.progress(90, text="æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
                summary = summarize_with_llm(
                    user_question=user_question, 
                    scraped_data=all_scraped_content,
                    api_key=st.session_state.api_key,
                    model_name=st.session_state.model_name,
                    base_url=st.session_state.base_url
                )
                my_bar.progress(100, text="å®Œæˆï¼")
                my_bar.empty() # æ¸…é™¤è¿›åº¦æ¡

                if summary:
                    st.session_state.answer_content = summary
                    answer_display_area.markdown(summary)
                else:
                    results_area.error("ç”Ÿæˆæ‘˜è¦å¤±è´¥ã€‚")
            except Exception as e:
                logging.error("å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸: %s", e, exc_info=True)
                results_area.error("å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°æ—¥å¿—ã€‚")
                my_bar.empty() # ç¡®ä¿é”™è¯¯æ—¶ä¹Ÿæ¸…é™¤è¿›åº¦æ¡
        except Exception as e:
            logging.error("å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸: %s", e, exc_info=True)
            st.error("å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°æ—¥å¿—ã€‚")

# --- é¡µè„šå’Œå…è´£å£°æ˜ ---
st.markdown("---")
st.markdown("""
<div style="text-align: center; font-size: small;">
    <p><strong>å…è´£å£°æ˜:</strong> æœ¬ç¨‹åºæ˜¯ä¸€ä¸ªç”± AI é©±åŠ¨çš„è¾…åŠ©å·¥å…·ã€‚æ‰€æœ‰ä¿¡æ¯å‡ä»ç½‘ç»œæœç´¢ç»“æœä¸­è‡ªåŠ¨æ‘˜è¦ï¼Œä¸ä¿è¯å…¶ 100% çš„å‡†ç¡®æ€§ã€‚è¯·é€šè¿‡æä¾›çš„æ¥æºé“¾æ¥æ ¸å®å…³é”®ä¿¡æ¯ã€‚</p>
    <p><strong>éšç§ä¿æŠ¤:</strong> æ‚¨çš„æŸ¥è¯¢å°†è¢«åŒ¿åå¤„ç†ï¼Œæˆ‘ä»¬ä¸ä¼šå­˜å‚¨ä»»ä½•æŸ¥è¯¢è®°å½•ã€‚</p>
</div>
""", unsafe_allow_html=True)
